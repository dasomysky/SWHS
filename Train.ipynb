{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb99c19-4866-45f1-a5b5-8becf07b3988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T13:06:04.055493Z",
     "iopub.status.busy": "2024-07-14T13:06:04.054589Z",
     "iopub.status.idle": "2024-07-14T13:06:22.273342Z",
     "shell.execute_reply": "2024-07-14T13:06:22.271885Z",
     "shell.execute_reply.started": "2024-07-14T13:06:04.055410Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-14 22:06:18.996052: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-14 22:06:19.045625: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-14 22:06:19.045649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-14 22:06:19.047024: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-14 22:06:19.055894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets, load_dataset, DatasetDict\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Iterator, Callable, Union, Tuple\n",
    "import re\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, CLIPTextModel, CLIPTokenizer, CLIPTextModelWithProjection, CLIPProcessor, CLIPModel\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AutoProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import clip\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a75de56a-debd-4490-a3bf-cc649d9b0d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T13:06:22.276049Z",
     "iopub.status.busy": "2024-07-14T13:06:22.275265Z",
     "iopub.status.idle": "2024-07-14T13:06:26.565160Z",
     "shell.execute_reply": "2024-07-14T13:06:26.563512Z",
     "shell.execute_reply.started": "2024-07-14T13:06:22.276019Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "ds = datasets.load_dataset('kimdeoldeol/COCO_dataset')\n",
    "# 데이터 소스 경로\n",
    "source_path = '/workspace/cache_kkuldanji/747f1772-e403-4641-a3eb-d57c6a8cbaea/source/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d106353a-22d1-404d-a599-4c25384f1365",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:36:37.974621Z",
     "iopub.status.busy": "2024-07-12T02:36:37.974196Z",
     "iopub.status.idle": "2024-07-12T02:36:51.643269Z",
     "shell.execute_reply": "2024-07-12T02:36:51.641810Z",
     "shell.execute_reply.started": "2024-07-12T02:36:37.974581Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevor/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:830: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# clip 모델로드\n",
    "model = AutoModel.from_pretrained('laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\n",
    "tokenizer = AutoTokenizer.from_pretrained('laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\n",
    "processor = AutoProcessor.from_pretrained('laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a21a3c80-fc64-4eea-8376-be89ce2e1439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:36:51.645782Z",
     "iopub.status.busy": "2024-07-12T02:36:51.645535Z",
     "iopub.status.idle": "2024-07-12T02:36:55.574411Z",
     "shell.execute_reply": "2024-07-12T02:36:55.572928Z",
     "shell.execute_reply.started": "2024-07-12T02:36:51.645760Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 1280)\n",
       "      (position_embedding): Embedding(77, 1280)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1664)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-47): 48 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            (v_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            (q_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            (out_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1664, out_features=8192, bias=True)\n",
       "            (fc2): Linear(in_features=8192, out_features=1664, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1664, out_features=1280, bias=False)\n",
       "  (text_projection): Linear(in_features=1280, out_features=1280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9394333c-6f28-4697-a428-087e05d8684c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:36:55.576215Z",
     "iopub.status.busy": "2024-07-12T02:36:55.575885Z",
     "iopub.status.idle": "2024-07-12T02:36:55.584650Z",
     "shell.execute_reply": "2024-07-12T02:36:55.583456Z",
     "shell.execute_reply.started": "2024-07-12T02:36:55.576182Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a custom dataset 데이터 전처리 함수\n",
    "class image_title_dataset():\n",
    "    def __init__(self, list_image_path,list_txt,tokenizer,processor):\n",
    "        # Initialize image paths and corresponding texts\n",
    "        self.image_path = list_image_path\n",
    "        \n",
    "        # self.title  = tokenizer(list_txt)\n",
    "        self.title  = clip.tokenize(list_txt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess image using CLIP's preprocessing function\n",
    "        \n",
    "        image = processor(images=Image.open(self.image_path[idx]),return_tensors=\"pt\").pixel_values.squeeze()\n",
    "        title = self.title[idx]\n",
    "        return image, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd8f64d-f795-43ca-8a7a-b660745ed3d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:36:55.586093Z",
     "iopub.status.busy": "2024-07-12T02:36:55.585861Z",
     "iopub.status.idle": "2024-07-12T02:37:00.310496Z",
     "shell.execute_reply": "2024-07-12T02:37:00.308676Z",
     "shell.execute_reply.started": "2024-07-12T02:36:55.586070Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use your own data\n",
    "data = []\n",
    "for i in ds['train']:\n",
    "    data.append(i)\n",
    "    \n",
    "train_img_paths, test_img_paths = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "pre_train_list = []\n",
    "pre_test_list = []\n",
    "train_list_image_path = []\n",
    "train_list_txt = []\n",
    "test_list_image_path = []\n",
    "test_list_txt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8380f5ab-8477-4629-b342-6fd4389a255f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:37:00.312300Z",
     "iopub.status.busy": "2024-07-12T02:37:00.312032Z",
     "iopub.status.idle": "2024-07-12T02:37:00.323390Z",
     "shell.execute_reply": "2024-07-12T02:37:00.321003Z",
     "shell.execute_reply.started": "2024-07-12T02:37:00.312275Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110030"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "471256e5-2fb0-49fd-ba72-0741cd686c72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:37:00.327350Z",
     "iopub.status.busy": "2024-07-12T02:37:00.326489Z",
     "iopub.status.idle": "2024-07-12T02:37:00.346261Z",
     "shell.execute_reply": "2024-07-12T02:37:00.344074Z",
     "shell.execute_reply.started": "2024-07-12T02:37:00.327275Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12226"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5bbe392-8e1f-4545-89df-7f9de05ffc6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:37:00.350042Z",
     "iopub.status.busy": "2024-07-12T02:37:00.349126Z",
     "iopub.status.idle": "2024-07-12T02:38:03.047929Z",
     "shell.execute_reply": "2024-07-12T02:38:03.044289Z",
     "shell.execute_reply.started": "2024-07-12T02:37:00.349969Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/110030 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (82 > 77). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 110030/110030 [01:02<00:00, 1755.84it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(train_img_paths):\n",
    "    image_path = source_path + item['source'].split('/')[-1]\n",
    "    \n",
    "    for i in item['caption_ko']:\n",
    "        caption = i\n",
    "        tokens = processor.tokenizer.tokenize(caption)\n",
    "        num_token = len(tokens)\n",
    "        \n",
    "        if num_token <=75: \n",
    "            data_dict={\"source\":image_path,\"caption\":caption}\n",
    "            pre_train_list.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c24d0a-676c-4267-8fd1-20c47aaf3c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:38:03.059403Z",
     "iopub.status.busy": "2024-07-12T02:38:03.058403Z",
     "iopub.status.idle": "2024-07-12T02:38:03.072406Z",
     "shell.execute_reply": "2024-07-12T02:38:03.070708Z",
     "shell.execute_reply.started": "2024-07-12T02:38:03.059332Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545170"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pre_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8893d12f-be88-4fb2-9e3d-48ad907b05ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:38:03.076441Z",
     "iopub.status.busy": "2024-07-12T02:38:03.075639Z",
     "iopub.status.idle": "2024-07-12T02:38:03.092618Z",
     "shell.execute_reply": "2024-07-12T02:38:03.089867Z",
     "shell.execute_reply.started": "2024-07-12T02:38:03.076355Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '/workspace/cache_kkuldanji/747f1772-e403-4641-a3eb-d57c6a8cbaea/source/COCO_train2014_000000337886.jpg',\n",
       " 'caption': '시내에 오토바이 경찰이 죽 늘어서 있다'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbebd436-c96e-443a-b65c-6b3517781e24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:38:03.097139Z",
     "iopub.status.busy": "2024-07-12T02:38:03.095802Z",
     "iopub.status.idle": "2024-07-12T02:38:03.814420Z",
     "shell.execute_reply": "2024-07-12T02:38:03.812211Z",
     "shell.execute_reply.started": "2024-07-12T02:38:03.097063Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pre_train_list)\n",
    "df_no_duplicates = df.drop_duplicates(subset='caption')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e01e6ca-e52e-4bed-975c-cb617ad7dbeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:38:03.817807Z",
     "iopub.status.busy": "2024-07-12T02:38:03.817062Z",
     "iopub.status.idle": "2024-07-12T02:38:03.981350Z",
     "shell.execute_reply": "2024-07-12T02:38:03.979872Z",
     "shell.execute_reply.started": "2024-07-12T02:38:03.817734Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511206\n",
      "511206\n"
     ]
    }
   ],
   "source": [
    "train_list_image_path = list(df_no_duplicates['source'])\n",
    "train_list_txt = list(df_no_duplicates['caption'])\n",
    "\n",
    "print(len(train_list_image_path))\n",
    "print(len(train_list_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb6c550d-5b3e-4696-b10a-92fd3b88f325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:38:03.982886Z",
     "iopub.status.busy": "2024-07-12T02:38:03.982642Z",
     "iopub.status.idle": "2024-07-12T02:38:10.982417Z",
     "shell.execute_reply": "2024-07-12T02:38:10.980294Z",
     "shell.execute_reply.started": "2024-07-12T02:38:03.982863Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12226/12226 [00:06<00:00, 1750.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(test_img_paths):\n",
    "    image_path = source_path + item['source'].split('/')[-1]\n",
    "    \n",
    "    for i in item['caption_ko']:\n",
    "        caption = i\n",
    "        tokens = processor.tokenizer.tokenize(caption)\n",
    "        num_token = len(tokens)\n",
    "        \n",
    "        if num_token <=75: \n",
    "            data_dict={\"source\":image_path,\"caption\":caption}\n",
    "            pre_test_list.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b68cc2d9-b3ed-4803-ac3e-05f1eddf5b16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:38:10.985613Z",
     "iopub.status.busy": "2024-07-12T02:38:10.985276Z",
     "iopub.status.idle": "2024-07-12T02:38:11.125194Z",
     "shell.execute_reply": "2024-07-12T02:38:11.123369Z",
     "shell.execute_reply.started": "2024-07-12T02:38:10.985576Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pre_test_list)\n",
    "df_no_duplicates = df.drop_duplicates(subset='caption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "001a049f-e30f-47bc-9acd-08220974b026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:38:11.128623Z",
     "iopub.status.busy": "2024-07-12T02:38:11.127601Z",
     "iopub.status.idle": "2024-07-12T02:38:11.168257Z",
     "shell.execute_reply": "2024-07-12T02:38:11.166852Z",
     "shell.execute_reply.started": "2024-07-12T02:38:11.128545Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59159\n",
      "59159\n"
     ]
    }
   ],
   "source": [
    "test_list_image_path = list(df_no_duplicates['source'])\n",
    "test_list_txt = list(df_no_duplicates['caption'])\n",
    "\n",
    "print(len(test_list_image_path))\n",
    "print(len(test_list_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d92f2c2c-ace7-435d-9e2a-461c708a1801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:38:11.169794Z",
     "iopub.status.busy": "2024-07-12T02:38:11.169495Z",
     "iopub.status.idle": "2024-07-12T02:39:32.714347Z",
     "shell.execute_reply": "2024-07-12T02:39:32.712585Z",
     "shell.execute_reply.started": "2024-07-12T02:38:11.169767Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = image_title_dataset(train_list_image_path, train_list_txt,tokenizer,processor)\n",
    "test_dataset = image_title_dataset(test_list_image_path, test_list_txt,tokenizer,processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a66518f8-d259-466d-a4cd-d1fd700f3725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:39:32.716010Z",
     "iopub.status.busy": "2024-07-12T02:39:32.715760Z",
     "iopub.status.idle": "2024-07-12T02:39:32.723680Z",
     "shell.execute_reply": "2024-07-12T02:39:32.722078Z",
     "shell.execute_reply.started": "2024-07-12T02:39:32.715985Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCH = 1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e6a4eba-9043-40ab-9900-d274fa6ec525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:39:32.725162Z",
     "iopub.status.busy": "2024-07-12T02:39:32.724914Z",
     "iopub.status.idle": "2024-07-12T02:39:32.739194Z",
     "shell.execute_reply": "2024-07-12T02:39:32.737150Z",
     "shell.execute_reply.started": "2024-07-12T02:39:32.725140Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to convert model's parameters to FP32 format\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55516fce-3276-406e-af96-c0507db2a5f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:39:32.744077Z",
     "iopub.status.busy": "2024-07-12T02:39:32.742173Z",
     "iopub.status.idle": "2024-07-12T02:39:32.752942Z",
     "shell.execute_reply": "2024-07-12T02:39:32.750484Z",
     "shell.execute_reply.started": "2024-07-12T02:39:32.743993Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d50efef-7775-4926-aedf-ca9e979c8230",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:39:32.756800Z",
     "iopub.status.busy": "2024-07-12T02:39:32.756058Z",
     "iopub.status.idle": "2024-07-12T02:39:32.778832Z",
     "shell.execute_reply": "2024-07-12T02:39:32.777079Z",
     "shell.execute_reply.started": "2024-07-12T02:39:32.756730Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5,betas=(0.9,0.999),eps=1e-6)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader)*EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78547f3b-6409-4249-920a-751b5988539f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:39:32.781832Z",
     "iopub.status.busy": "2024-07-12T02:39:32.780764Z",
     "iopub.status.idle": "2024-07-12T02:39:32.789208Z",
     "shell.execute_reply": "2024-07-12T02:39:32.787461Z",
     "shell.execute_reply.started": "2024-07-12T02:39:32.781787Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the loss function\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd33f1b1-d72f-4fe0-ba4e-7bd01dd18245",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:39:32.791505Z",
     "iopub.status.busy": "2024-07-12T02:39:32.791071Z",
     "iopub.status.idle": "2024-07-12T12:48:51.479426Z",
     "shell.execute_reply": "2024-07-12T12:48:51.476984Z",
     "shell.execute_reply.started": "2024-07-12T02:39:32.791464Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch 1, best test loss 100000.0 after epoch -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batchCE: 0.002969533670693636: 100%|██████████| 31951/31951 [9:40:38<00:00,  1.09s/it]    \n",
      "test batchCE: 0.00013090771972201765: 100%|██████████| 3698/3698 [26:31<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, tr_loss 9.294714396925556e-08, te_loss 4.1360564738313094e-08\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = EPOCH\n",
    "\n",
    "best_te_loss = 1e5\n",
    "best_ep = -1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"running epoch {num_epochs}\")\n",
    "    step = 0\n",
    "    tr_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    train_pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    \n",
    "    for batch in train_pbar:\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        # images,texts = batch\n",
    "        texts,images = batch\n",
    "        \n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(images, texts)\n",
    "        logits_per_image = output.logits_per_image\n",
    "        logits_per_text = output.logits_per_text\n",
    "\n",
    "        # Compute loss\n",
    "        # ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        \n",
    "   \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        tr_loss += total_loss.item()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # clip.model.convert_weights(model)\n",
    "\n",
    "        # pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "        # pbar.set_description(f\"train : {total_loss.item()}\",refresh=True)\n",
    "        train_pbar.set_description(f\"train batchCE: {total_loss.item()}\", refresh=True)\n",
    "        \n",
    "        tr_loss /= step\n",
    "    \n",
    "    \n",
    "    step=0\n",
    "    te_loss=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_pbar = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "        \n",
    "        for batch in test_pbar:\n",
    "            step +=1\n",
    "            texts,images  = batch\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "            \n",
    "            output = model(images, texts)\n",
    "            logits_per_image = output.logits_per_image\n",
    "            logits_per_text = output.logits_per_text\n",
    "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "            \n",
    "            total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth))/2\n",
    "            te_loss += total_loss.item()\n",
    "            test_pbar.set_description(f\"test batchCE: {total_loss.item()}\", refresh=True)\n",
    "            \n",
    "            te_loss /= step\n",
    "        \n",
    "        \n",
    "        if te_loss < best_te_loss:\n",
    "            best_te_loss = te_loss\n",
    "            best_ep = epoch\n",
    "            # torch.save(model.state_dict(), 'model/eval_test.pt')\n",
    "            \n",
    "            model.save_pretrained(f'/mnt/share/bcm/trevor/stable_diffusion/test_model_{epoch}')\n",
    "        print(f\"epoch {epoch}, tr_loss {tr_loss}, te_loss {te_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
